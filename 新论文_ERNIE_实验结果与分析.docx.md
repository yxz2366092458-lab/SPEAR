# ERNIE: 基于对抗正则化的鲁棒多智能体强化学习算法

## 第3章 实验设计与分析

### 3.3 算法性能评估

#### 3.3.3 实验结果与分析

##### 3.3.3.1 实验设置

**环境配置**
本实验在交通信号控制网格环境中进行，采用SUMO仿真平台。具体配置如下：

- **环境类型**：2×2交通信号网格
- **智能体数量**：4个交通信号灯控制器
- **观察空间**：每个智能体22维观察向量（18个交通特征 + 4个one-hot智能体ID）
- **动作空间**：离散动作空间{0, 1}，表示信号灯状态切换
- **奖励函数**：基于队列长度和等待时间的负奖励
- **仿真参数**：步长1秒，总时长3600秒，交通流量500-800辆/小时

**对比算法**
实验对比五种多智能体强化学习算法：
1. **QCOMBO**：基于值分解的算法
2. **COMA**：反事实多智能体策略梯度算法  
3. **MADDPG**：多智能体深度确定性策略梯度算法
4. **MAPPO**：多智能体近端策略优化算法
5. **ERNIE**：基于QCOMBO的对抗正则化增强版本

**训练参数**
- 训练episode数：3000
- 评估episode数：5000
- 随机种子：0（确保可重复性）
- 折扣因子：γ = 0.99
- 批量大小：32（MADDPG/MAPPO）
- 学习率：各算法采用其最优配置

##### 3.3.3.2 性能对比结果

**训练曲线分析**
五种算法的训练曲线如图3.5所示。ERNIE算法表现出最佳的综合性能：
- **收敛速度**：MAPPO最快（约500 episode），MADDPG次之（约800 episode）
- **最终性能**：ERNIE最优（平均奖励-12.3），排名：ERNIE > MAPPO > MADDPG > QCOMBO > COMA
- **稳定性**：MAPPO和ERNIE训练曲线最平滑，MADDPG初期有波动

**最终性能统计**
表3.3：算法性能对比

| 算法 | 平均奖励 | 标准差 | 最大奖励 | 最小奖励 | 收敛episode |
|------|----------|--------|----------|----------|-------------|
| QCOMBO | -18.7 | 2.3 | -15.2 | -22.1 | 1200 |
| COMA | -21.4 | 2.8 | -17.8 | -25.6 | 1500 |
| MADDPG | -15.6 | 1.9 | -12.4 | -18.9 | 800 |
| MAPPO | -14.2 | 1.7 | -11.5 | -17.3 | 500 |
| ERNIE | **-12.3** | **1.5** | **-9.8** | **-15.1** | 600 |

**统计显著性检验**
使用配对t检验（α=0.05）验证性能差异显著性：
- ERNIE vs. MAPPO：p=0.032 < 0.05，差异显著
- ERNIE vs. MADDPG：p=0.021 < 0.05，差异显著  
- ERNIE vs. QCOMBO：p=0.008 < 0.05，差异显著
- ERNIE vs. COMA：p=0.003 < 0.05，差异显著

##### 3.3.3.3 算法特性分析

**探索效率对比**
通过累积新颖状态数量衡量探索效率：
- MAPPO：1250个状态（熵正则化保持高探索）
- ERNIE：1100个状态（对抗正则化增强探索）
- MADDPG：980个状态（OU噪声探索）
- QCOMBO：750个状态
- COMA：620个状态

**样本效率分析**
达到90%最终性能所需训练样本：
- MAPPO：45,000样本（效率最高）
- ERNIE：55,000样本
- MADDPG：60,000样本
- QCOMBO：85,000样本
- COMA：95,000样本

**计算复杂度比较**
单次迭代平均时间（NVIDIA RTX 3090）：
- QCOMBO：0.8ms（最低）
- COMA：0.9ms
- MADDPG：1.2ms
- MAPPO：1.5ms
- ERNIE：1.8ms（对抗正则化增加计算）

##### 3.3.3.4 消融实验

**ERNIE组件分析**
验证ERNIE各组件有效性：
- 完整ERNIE：平均奖励-12.3
- 无对抗正则化：-14.7（下降19.5%）
- 无课程学习：-13.8（下降12.2%）
- 无自适应权重：-13.1（下降6.5%）

对抗正则化贡献约65%性能增益，是主要提升来源。

**MADDPG超参数敏感性**
- 目标网络更新率τ：τ=0.01最优，τ=0.1导致不稳定
- OU噪声参数：θ=0.15, σ=0.2最佳，减小噪声性能下降12%

**MAPPO裁剪系数分析**
PPO裁剪系数ε影响：
- ε=0.2：标准设置，性能最优
- ε=0.1：过度约束，性能下降8%
- ε=0.3：约束不足，方差增加42%

##### 3.3.3.5 鲁棒性测试

**环境扰动测试**
1. 观测噪声（σ=0.2）：
   - ERNIE性能下降15%
   - 其他算法下降25-35%

2. 动作延迟（3步延迟）：
   - ERNIE下降22%
   - 其他算法下降30-45%

3. 通信故障（30%丢包）：
   - ERNIE下降18%
   - 其他算法下降40-55%

**迁移学习测试**
2×2网格训练模型迁移到3×3网格：
- 零样本迁移：ERNIE成功率65%（最优）
- 少样本微调（100 episode）：
  - ERNIE：达到目标性能92%
  - MAPPO：85%
  - MADDPG：78%
  - QCOMBO/COMA：<70%

##### 3.3.3.6 可视化分析

**策略可视化**
训练后策略显示：
- ERNIE：策略最协调，智能体合作有效
- MAPPO：协调性良好，少量冲突
- MADDPG：个体策略优，全局协调稍差
- QCOMBO/COMA：策略相对简单

**注意力机制分析**
ERNIE的对抗注意力在关键决策时刻更集中，形成动态注意力分配模式。

##### 3.3.3.7 讨论与启示

**算法选择建议**
1. **追求最高性能**：选择ERNIE，适合鲁棒性要求高场景
2. **样本效率优先**：选择MAPPO，适合数据收集成本高场景
3. **计算资源有限**：选择QCOMBO，计算复杂度最低
4. **需要理论保证**：COMA具有较好理论性质

**实际应用考虑**
- ERNIE对抗正则化增强对异常交通流量适应性
- MAPPO稳定性适合长期部署
- MADDPG连续动作空间扩展性更好
- 需权衡通信成本和计算延迟

**局限性分析**
1. 仅在模拟环境验证，需真实环境测试
2. 网格规模较小，需扩展到大系统
3. 假设完全可观察，实际可能存在部分可观察性
4. 通信模型较理想，实际可能存在带宽限制

##### 3.3.3.8 总结

本章通过系统实验对比了五种多智能体强化学习算法在交通信号控制任务中的性能，主要结论：

1. **ERNIE算法综合性能最优**，在平均奖励、鲁棒性和迁移能力方面均表现最佳，验证了对抗正则化的有效性。

2. **MAPPO算法样本效率最高**，适合数据有限的场景，且训练稳定性好。

3. **MADDPG算法在连续动作扩展方面具有优势**，为后续研究提供基础。

4. **传统算法（QCOMBO、COMA）** 在计算资源受限时仍具有实用价值。

5. **算法性能与计算复杂度存在权衡**，实际应用需根据需求选择。

实验结果验证了各算法有效性，为多智能体强化学习在交通控制等实际场景中的应用提供了重要参考。对抗正则化被证明是提升算法鲁棒性的有效方法，为未来研究提供了新方向。

---

**关键词**：多智能体强化学习，交通信号控制，对抗正则化，算法对比，鲁棒性，迁移学习

**图目录**：
- 图3.5：五种算法训练曲线对比
- 图3.6：算法探索效率对比
- 图3.7：训练后策略可视化

**表目录**：
- 表3.3：算法性能对比
- 表3.4：样本效率对比
- 表3.5：计算复杂度对比
- 表3.6：ERNIE消融实验结果